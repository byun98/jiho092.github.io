---
layout: single
title: "One-Step Image Translation with Text-to-Image Models" #제목
excerpt : ""
categories: 
    - deeplearningCV #카테고리설정
tag: 
    - [] #테그

date: 2025-01-31
last_modified_at: 2025-01-31
classes: wide    
---
[One-Step Image Translation with Text-to-Image Models](https://arxiv.org/pdf/2403.12036){: .btn .btn--primary}


# 0. Abstract

논문에서는 기존 conditional diffusion model(Image translation task)에서 두가지 한계점으로 인한 느린 inference speed를 극복하려한다.

* 두가지 한계점
    1. 반복적인 denoising 과정
    2. paired 데이터에 의존해야함

* 해결 방법
    1. 기존의 latent diffusion model의 다양한 module들을 하나로 통합하여 end-to-end generator로 통합시킴
    2. 적은 trainable한 weight를 추가하여 input image의 구조를 유지하며 overfitting을 방지

이 논문은 unpaired data에 대해서는 CycleGAN-turbo model을 사용하고, paired data에 대해서는 pix2pix-turbo model을 제시하였으며, 성능은 GAN based, diffusion based 보다 우수한 성능을 보였다.

# 1. Introduction

![Image5](/assets/images/img2img/image1.jpg){: .align-center}


&nbsp;&nbsp; conditional diffusion model은 user가 이미지를 생성하는데 spatial conditioning, text prompts를 이용할 수 있기 때문에, scene layout, user sketches, human pose 등 다양한 image 합성에 사용이 가능했다. 그러나 이 모델에는 두가지 challenge가 존재한다.

    1. Diffusion model은 반복을 통해 진행되기 때문에 inference 속도가 매우 느리다.
    2. 학습을 위해 paired 데이터를 수집해야하는데, 이것은 많은 cost가 발생한다.

&nbsp;&nbsp; 이러한 문제를 해결하기 위해 논문에서는 paired, unpaired dataset에 모두 적용 가능한 **one-step image-to-image translation** 방법을 제안한다. 이 방법은 기존의 CDM(conditional diffusion model)과 비교가능한 결과를 내고, 1번의 step으로 진행되기 떄문에 추론 속도가 빠르다. 가장 중요한점은 paired image set 없이도 학습이 가능하다는 점이다. 논문에서의 핵심아이디어는 SD-Turbo와 같은 충분히 pre-train된 데이터를 GAN과 같은 방법을 이용하여 적용하는 것이다. 

&nbsp;&nbsp; 기존의 diffusion model들과 달리, 논문의 저자들은 one-step model에서 <span style="color:red"> noise map </span>이 직접적으로 output 구조에 영향을 준다는 것을 발견하였다. 결과적으로 <span style="color:red"> adapter branch </span>를 추가하여 noise map과 input conditioning을 모두 먹이면 네트워크에 대한 정보가 충돌한다.

게다가 image-to-image translation에서 대부분 visual detail이 Encoder-Decoder 구조와 같은 pipeline에서 손상된다. 이 detail에 대한 loss는 day-to-night translation에서 아주 중요한 부분이다.

&nbsp;&nbsp; 이 논문에서는 새로운 generator architecture를 제안하는데 3가지 idea를 통해 image detail의 손상을 방지한다. **첫 번쨰는 conditioning information을 직접적으로 U-net noise encoder branch에 먹이는 것이다.** 이 방법을 통해 network가 noise map과 input control간의 충돌을 방지할 수 있다. **두 번쨰는 3개로 분리되어 있는 Encoder, Unet, Decoder를 학습가능한 end-to-end 방법으로 통합시켰다.** 이 방법에서 <span style="color:red"> LoRA </span>을 채용하여, overfitting을 방지하고, fine-tuning time을 감소시켰다. **마지막으로 고해상도를 유지하기 위해 encoder와 decoder에 zero-conv를 통한 Skip-Connection을 시켰다.** 

&nbsp;&nbsp; 논문에서는 주로 day-to-night, adding/removing weather effect와 같은 unpaired image를 다루고, 이 방법이 GAN, Diffusion 방법과 비교했을 때, 구조 보존을 잘한다는 것을 알아냈다. 


# 2. Related Work

1. Image-to-Image translation

2. Text-to-Image models

3. One-step gernerative models

# 3. Method

&nbsp;&nbsp; 논문에서는 안정적으로 realistic image를 생성할 수 있는 one step pretrained text-to-image model에서 시작한다. 이 저자들의 목표는 real image input을 source domain에서 target domain으로 변환 시키는 것이다. **3.1절**에서는 다른 conditioning methods 방법들을 보면서 관련된 challenge들을 해결하기 위해 추가한 구조들을 살펴보고, **3.2절**에서는 일반적은 detail loss 문제(e.g text, hands, street signs)에 대해 연구하고, 해결책을 제안한다. 이후 **3.3, 3.4절**에서는 논문에서의 unpaired, paired image translation에 대해 이야기한다. 

![Image5](/assets/images/img2img/image2.jpg){: .align-center}

## 3.1 Adding Conditioning Input

&nbsp;&nbsp; text-to-image model을 image translation model로 전환시키기 위해 먼저 input image *x*를 model에 통합시키는 효율적인 방법을 찾아야한다. 

1. Conflict between noise and conditional input

conditional input을 Diffusion model에 통합시키는 일반적인 전략은 extra adapter branches를 도입하는 것이다. 위 그림과 같이 논문에서는 Stable diffusion Encoder의 weight를 사용하거나, 가벼운 network를 random하게 초기화하여 Conditional Encoder로 labeling된 두 번쨰 Encoder를 초기화 한다.