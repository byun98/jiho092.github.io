---
layout: single
title: "One-Step Image Translation with Text-to-Image Models" #제목
excerpt : ""
categories: 
    - deeplearningCV #카테고리설정
tag: 
    - [] #테그

date: 2025-01-31
last_modified_at: 2025-01-31
classes: wide    
---
[One-Step Image Translation with Text-to-Image Models](https://arxiv.org/pdf/2403.12036){: .btn .btn--primary}


# 0. Abstract

논문에서는 기존 conditional diffusion model(Image translation task)에서 두가지 한계점으로 인한 느린 inference speed를 극복하려한다.

* 두가지 한계점
    1. 반복적인 denoising 과정
    2. paired 데이터에 의존해야함

* 해결 방법
    1. 기존의 latent diffusion model의 다양한 module들을 하나로 통합하여 end-to-end generator로 통합시킴
    2. 적은 trainable한 weight를 추가하여 input image의 구조를 유지하며 overfitting을 방지

이 논문은 unpaired data에 대해서는 CycleGAN-turbo model을 사용하고, paired data에 대해서는 pix2pix-turbo model을 제시하였으며, 성능은 GAN based, diffusion based 보다 우수한 성능을 보였다.

# 1. Introduction

![Image5](/assets/images/img2img/image1.jpg){: .align-center}


&nbsp;&nbsp; conditional diffusion model은 user가 이미지를 생성하는데 spatial conditioning, text prompts를 이용할 수 있기 때문에, scene layout, user sketches, human pose 등 다양한 image 합성에 사용이 가능했다. 그러나 이 모델에는 두가지 challenge가 존재한다.

1. Diffusion model은 반복을 통해 진행되기 때문에 inference 속도가 매우 느리다.
2. 학습을 위해 paired 데이터를 수집해야하는데, 이것은 많은 cost가 발생한다.

&nbsp;&nbsp; 이러한 문제를 해결하기 위해 논문에서는 paired, unpaired dataset에 모두 적용 가능한 **one-step image-to-image translation** 방법을 제안한다. 이 방법은 기존의 CDM(conditional diffusion model)과 비교가능한 결과를 내고, 1번의 step으로 진행되기 떄문에 추론 속도가 빠르다. 가장 중요한점은 paired image set 없이도 학습이 가능하다는 점이다. 논문에서의 핵심아이디어는 SD-Turbo와 같은 충분히 pre-train된 데이터를 GAN과 같은 방법을 이용하여 적용하는 것이다. 

&nbsp;&nbsp; 기존의 diffusion model들과 달리, 논문의 저자들은 one-step model에서 <span style="color:red"> noise map </span>이 직접적으로 output 구조에 영향을 준다는 것을 발견하였다. 결과적으로 <span style="color:red"> adapter branch </span>를 추가하여 noise map과 input conditioning을 모두 먹이면 네트워크에 대한 정보가 충돌한다.

게다가 image-to-image translation에서 대부분 visual detail이 Encoder-Decoder 구조와 같은 pipeline에서 손상된다. 이 detail에 대한 loss는 day-to-night translation에서 아주 중요한 부분이다.

&nbsp;&nbsp; 이 논문에서는 새로운 generator architecture를 제안하는데 3가지 idea를 통해 image detail의 손상을 방지한다. **첫 번쨰는 conditioning information을 직접적으로 U-net noise encoder branch에 먹이는 것이다.** 이 방법을 통해 network가 noise map과 input control간의 충돌을 방지할 수 있다. **두 번쨰는 3개로 분리되어 있는 Encoder, Unet, Decoder를 학습가능한 end-to-end 방법으로 통합시켰다.** 이 방법에서 <span style="color:red"> LoRA </span>을 채용하여, overfitting을 방지하고, fine-tuning time을 감소시켰다. **마지막으로 고해상도를 유지하기 위해 encoder와 decoder에 zero-conv를 통한 Skip-Connection을 시켰다.** 

&nbsp;&nbsp; 논문에서는 주로 day-to-night, adding/removing weather effect와 같은 unpaired image를 다루고, 이 방법이 GAN, Diffusion 방법과 비교했을 때, 구조 보존을 잘한다는 것을 알아냈다. 


# 2. Related Work

1. Image-to-Image translation

2. Text-to-Image models

3. One-step gernerative models

# 3. Method

&nbsp;&nbsp; 논문에서는 안정적으로 realistic image를 생성할 수 있는 one step pretrained text-to-image model에서 시작한다. 이 저자들의 목표는 real image input을 source domain에서 target domain으로 변환 시키는 것이다. **3.1절**에서는 다른 conditioning methods 방법들을 보면서 관련된 challenge들을 해결하기 위해 추가한 구조들을 살펴보고, **3.2절**에서는 일반적은 detail loss 문제(e.g text, hands, street signs)에 대해 연구하고, 해결책을 제안한다. 이후 **3.3, 3.4절**에서는 논문에서의 unpaired, paired image translation에 대해 이야기한다. 

![Image5](/assets/images/img2img/image2.jpg){: .align-center}

## 3.1 Adding Conditioning Input

&nbsp;&nbsp; text-to-image model을 image translation model로 전환시키기 위해 먼저 input image $x$를 model에 통합시키는 효율적인 방법을 찾아야한다. 

### 1. Conflict between noise and conditional input
![Image5](/assets/images/img2img/image3.jpg){: .align-center}
&nbsp;&nbsp; conditional input을 Diffusion model에 통합시키는 일반적인 전략은 extra adapter branches를 도입하는 것이다. 위 그림과 같이 논문에서는 Stable diffusion Encoder의 weight를 사용하거나, 가벼운 network를 random하게 초기화하여 Conditional Encoder로 labeling된 두 번쨰 Encoder를 초기화 한다. 이 Control Encoder는 residual connection으로 pre-trained된 Stable Diffusion에 대해 $x$로 feature map을 반환한다. 이 방법은 diffusion model을 제어하는데 큰 효과를 보였는데, 여전히 위 그림과 같이 두 Encoder를 사용하여 noise map과 input image를 처리하는데 있어서 one step model의 context적 문제가 존재한다. multi diffusion 모델과 다르게 one step model의 noise map은 생성된 image가 input 이미지 구조와 모순되는 결과를 초래한다. 그러므로 Decoder는 두 개의 residual feature set을 받아야하여, training 과정이 더욱 어려워진다.

### 2. Direct conditioning input

&nbsp;&nbsp; 위 그림은 또한 pre-trained model에서 생성된 image가 noise map $z$에 큰 영향을 받는다는 것을 보여준다. 이를 바탕으로 network에 conditioning input을 직접 먹여야한다. Backbone model은 새로운 conditioning을 허락하고, 논문에서는 **LoRA** weight 몇가지를 가져와서 다양한 Unet layer에 추가한다.

## 3.2 Preserving input Details

&nbsp;&nbsp; LDM에서 의 주된 문제는 multi-object, complex scene에서 detail이 손상된다는 것이다. 

### 1. Detail이 손실되는 이유

![Image5](/assets/images/img2img/image4.jpg){: .align-center}
&nbsp;&nbsp; LDM은 image를 8배로 압축시키고, 채널수를 3개에서 4개로 늘린다. 이것은 training, inference 속도를 빠르게 해주지만, image translation에서는 detail을 보존해야하므로 이상적이지 않다. 따라서 논문에서는 **Skip Connection**을 적용한다. 위 그림을 보면 skip connection을 사용하지 않았을 때는, text, street sign, car의 detail이 손상되는 것을 확인할 수 있다. 반먄 skip connection을 도입했을 때는 이것들이 잘 유지되는 것을 볼 수 있다.


### 2. Connecting first stage encoder and decoder

&nbsp;&nbsp; input image의 visual적 detail을 잡기위해 Encoder, Decoder사이에 skip connection을 추가하였는데, 특히 Encoder 내에서 각 downsampling block에서 4개의 중간 activation을 추출하였고, 이것을 **1 by 1 zero-conv. layer**로 처리하여 연관되는 upsampling block에 먹인다. 이 방법은 image translation process에서 전반적으로 detail을 보장한다.

## 3.3 Unpaired Training

&nbsp;&nbsp; 논문에서는 모든 실험 network를 one step inference가 가능한 Stable diffusion Turbo(v2.1)을 사용한다. 이것은 CycleGAN과 같이 unpaired translation이 가능하다. 구체적으로 목표는 unpaired dataset $X, Y$가 주어졌을 때, source domain $X \subset \mathbb{R}^{H \times W \times 3}$를 target domain $Y \subset \mathbb{R}^{H \times W \times 3}$로 바꾸는 것이다. 또한 이 방법은 $G(x, c_Y) : X \to Y$, $G(y, c_X) : Y \to X$ 두 가지 translation function을 포함하고 있다. 두 translation모두 **같은 network $G$** 사용하지만, 다른 caption $c_X$, $c_Y$를 사용한다. 예를 들어 day $\to$ night에서는 $c_X$가 **Driving in the Day**이고, $c_Y$가 **Driving in the Night**이다. 그림 2처럼 **<span style="color:red">대부분의 layer를 Frozen 시키고, 첫 번째 Convolution layer와 추가된 LoRA adapter만 학습시킨다. </span>**

Loss에 대해 살펴보자.
### 1. Cycle consistency with perceptual loss

&nbsp;&nbsp; Cycle Consistency Loss $\mathcal{L}_{\text{cycle}}$는 각 source image $x$에서 두 개의 translation 함수로 스스로 돌아가게 하도록 영향을 준다. $\mathcal{L}_{\text{rec}}$는 $L1$과 LIPIPS의 결합이다. weight는 Appendix D에 나와있다.

$$
\mathcal{L}_{\text{cycle}} = \mathbb{E}_x \left[ \mathcal{L}_{\text{rec}}(G(G(x, c_Y), c_X), x) \right]  + \mathbb{E}_y \left[ \mathcal{L}_{\text{rec}}(G(G(y, c_X), c_Y), y) \right]
$$

### 2. Adversarial Loss
&nbsp;&nbsp; 논문에서는 output이 target domain과 일치하게 하기 위해 두 domain에 대해 모두 adversarial loss를 사용한다. 변환된 image와 실제 image를 분류하기 위해 두 Disciminator $\mathcal{D}_x$와 $\mathcal{D}_y$를 사용하고, Vision-Aided GAN을 따라 CLIP을 Backborn으로 사용한다. 

$$
\mathcal{L}_{\text{GAN}} = \mathbb{E}_y [\log D_Y(y)] + \mathbb{E}_x [\log (1 - D_Y(G(x, c_Y)))] + \mathbb{E}_y [\log D_X(x)] + \mathbb{E}_y [\log (1 - D_X(G(y, c_X)))]
$$

### 3. Full objective
&nbsp;&nbsp; 총 세 개의 loss를 사용한다. cycle consistency loss $\mathcal{L}_{\text{cycle}}$, adversarial loss $\mathcal{L}_{\text{GAN}}$, identity regularization loss $\mathcal{L}_{\text{idt}}$이다. 이 중 identity regularization loss는 $\mathcal{L}_{\text{idt}} = \mathbb{E}_x [\mathcal{L}_{\text{rec}}(G(x, c_X), x)] + \mathbb{E}_y [\mathcal{L}_{\text{rec}}(G(y, c_Y), y)]$이다. 

$$argmin_G\mathcal{L}_{\text{cycle}} + \lambda_{idt}\mathcal{L}_{idt} + \lambda_{GAN}\mathcal{L}_{\text{GAN}}$$

## 3.4 Entension

&nbsp;&nbsp; 이 논문에서 unpired learning에 초점을 두고 있지만, paired data로 학습하거나 출력하는데 있어서도 GAN을 학습하기 위한 두 가지 확장을 보여준다.

1. Paired training

2. Generating diverse outputs

# 4. Experiments

&nbsp;&nbsp; 실험에서는 image translation을 크게 세 가지 category로 나눈다. 첫 번째는 이 모델을 GAN-based, Diffusion-based와 비교하여 양적, 질적으로 좋다는 것을 증명할 것이고, 두 번째는 4.2절에서 unpaired method CycleGAN-Turbo의 모든 구성 성분의 효과를 분석한다. 마지막으로 extension 부분에 대해 보인다.

* Training Details
학습 가능한 parameter는 330MB이며, LoRA weight, zero-conv layer, Unet의 첫번째 conv layer를 포함한다.

* Dataset
    1. CycleGAN dataset : 256 X 256, (Horse to Zebra, Summer to Winter)
    2. BDD100k, DENSE : 512 X 512, (Day to Night, Clear to Foggy)

## 1. Comparison to baselines on driving dataset

![Image5](/assets/images/img2img/image5.jpg){: .align-center}
![Image5](/assets/images/img2img/image6.jpg){: .align-center}

FID Score로 보았을 때, Day to Night, Night to Day, Clear to Foggy, Foggy to Clear에서 CycleGAN-Turbo가 가장 좋은 성능을 보였고, DINO Struct. Score에 대해서도 Clear to Foggy, Foggy to Clear에서 좋은 성능을 보였다.

![Image5](/assets/images/img2img/image7.jpg){: .align-center}

다음은 사람의 선호도를 평가한 표이다. 이에 대해서도 논문의 모델이 좋은 평가를 받고 있다.

# 5. Dicussion and Limitation

&nbsp;&nbsp; 논문의 연구는 one step pre-trained model이 다양한 downstream image 합성에서 강력한 backborn model로 사용될 수 있음을 시사하였다. 이 모델은 새로운 task와 domain에 적용하는데에 있어, diffusion을 훈련 없이도 사용할 수 있고, 적은 수의 additional trainable parameter만을 필요로 한다.

## 1. 한계점

&nbsp;&nbsp; 논문의 모델이 visual적으로 매력적인 결과를 내지만 one-step으로는 한계점이 존재한다. 먼저 SD-turbo가 classsifier-free guidance를 사용하지 않기 때문에, guidance의 강도를 지정할 수 없다는 점이고, 두 번째는 negative prompt를 지원하지 않아서 artifact를 줄이지 못한다는 것이다. 또한 Cycle consistency loss와 generator가 용량이 크기 때문에, 메모리 사용량이 많다는 단점이 있다.