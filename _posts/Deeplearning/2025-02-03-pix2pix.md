---
layout: single
title: "Image-to-Image Translation with Conditional Adversarial Networks" #제목
excerpt : ""
categories: 
    - deeplearningCV #카테고리설정
tag: 
    - [] #테그

date: 2025-02-03
last_modified_at: 2025-02-03
classes: wide    
---
[Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/pdf/1611.07004){: .btn .btn--primary}


# Abstract

&nbsp;&nbsp; 이 논문에서는 image translation에 대한 일반적인 solution으로 사용하는 GAN에 대해 연구한다. 이 network는 input image to output image로의 mapping만 학습할 뿐만 아니라, mapping을 위한 loss function도 학습한다. 이는 다른 loss 공식이 필요한 문제에 대해서도 같은 포괄적인 접근을 가능하게 한다. 논문에서는 이를 통해 photos from labels map, reconstructing object from edge maps, colorizing 등 다양한 이미지 합성에서 효과적인 성능을 보이는 것을 증명했으며, **pix2pix** 출시 이후에 많은 사람들이 이 시스템을 사용하는 것을 보면서 parameter 조정 없이 다양한 곳에서 적용할 수 있음을 보였다. 

# 1. Introduction
![Image5](/assets/images/pix2pix/image1.jpg){: .align-center}

&nbsp;&nbsp; 많은 image processing, computer vision에서의 문제들은 input image를 관련된 output image로 변환하는 것이다. 이 개념은 언어를 번혁하는 것처럼 RGB, gradient field, edge map, semantic label map 등에서 다양하게 rendering 될 수 있다. 자동 언어 번역이 가능한 것처럼 논문에서는 충분한 training data가 주어졌을 때, 자동 image to image task를 정의하였다. 기존의 논문들에서는 이러한 다양한 task를 각 별도에 맞도록 처리하였다. 이 논문에서의 목표는 이러한 다양한 문제들에 대해 공통적인 framework를 개발하는 것이다. 

&nbsp;&nbsp; 이 연구 방안에서 CNN을 통한 광범위한 연구가 진행되었고, loss function을 최소화 하는 방법으로 좋은 성능을 보여왔다. 그러나 효과적인 loss를 design하기 위해서는 많은 작업이 필요하다. CNN을 predict와 ground truth의 Eclidean distance로 최소화한다면, 결과물이 흐려지는 경향이 있기 때문이다.(Eclidean distance는 결과의 모든 값을 평균화 하기 때문에 blur가 생김.)

&nbsp;&nbsp; 만약 reality와 구분할 수 없는 output을 만드는 것처럼 high level을 달성하기 위해 loss function을 적절하게 설정한다면, 매우 바람직할 것이다. GAN model은 이를 수행할 수 있으며, output이 real인지 fake인지 구분하는 discriminator loss와 generator loss를 사용하여 학습한다. 이 때, blur된 이미지는 fake로 보이기 때문에 선명한 image를 생성할 수 있게 된다. 논문에서는 GANs의 conditional setting에 대해 탐구하고, data 생성 모델을 학습하는 것처럼 Conditional GANs는 conditional 생성모델을 학습한다. 따라서 cGAN은 conditional input image에 따라서 output을 생성하는 image translation task에서 적합하다.

# 2. Related work

1. Strucured losses for image modeling

2. Conditional GANs

# 3. Method

![Image5](/assets/images/pix2pix/image2.jpg){: .align-center}

&nbsp;&nbsp; GANs은 random noise vector $z$로 output image $y$, $G : z \to y$ 로의  mapping을 학습하는 생성모델이다. 반면, conditional GANs는 관측 이미지 $x$, random noise vector $z$로 $y$, $G : (x,z) \to y$ 로의 mapping을 학습하는 모델이다.  G는 output이 real인지 fake인지 구분할 수 없도록 학습되고, D는 이것을 구분할 수 있도록 학습된다. 

## 3.1 Objective

$$
\mathcal{L}_{\text{cGAN}}(G, D) = \mathbb{E}_{x,y} [\log D(x, y)] + \mathbb{E}_{x,z} [\log (1 - D(x, G(x, z)))]
$$

&nbsp;&nbsp; conditioning Discriminator의 중요성을 test하기 위해, $x$를 고려하지 않는 unconditional도 함께 비교한다. 

$$
\mathcal{L}_{\text{GAN}}(G, D) = \mathbb{E}_{y} [\log D(y)] + \mathbb{E}_{x,z} [\log (1 - D(x, G(x, z)))]
$$

&nbsp;&nbsp; 이전의 접근법들은 전통적인 loss와 GAN의 objective를 조합하는 것이 유용하다고 나와있다. Discriminator의 역할은 변하지 않지만, generator의 task가 discriminator뿐만 아니라 ground truth image와 유사해야하기 때문에, L2 loss를 사용하는 것이 도움이 된다. 그러나 논문에서는 L2 loss보다 L1이 blurring이 적다는 것을 발견했기 때문에 최종적으로 generator에 L1 loss를 결합하여 사용한다. 

$$
\mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z} \left[ \| y - G(x, z) \|_1 \right].
$$

따라서 최종적인 목적함수는 다음과 같다.

$$
G^* = \arg \min_G \max_D \mathcal{L}_{\text{cGAN}}(G, D) + \lambda \mathcal{L}_{L1}(G)
$$

&nbsp;&nbsp; $z$ 없이 $x$를 $y$로의 mapping을 model이 학습할 수 있지만, 결정적인(determinstic) output을 생성하기 때문에 delta function의 분포로만 표현 가능하고, 다른 것들은 표현할 수 없다. 과거 Past Conditional GANs의 연구에서는 이것을 인정하고, Gaussian noise $z$를 추가하여 실험을 진행하였지만, 이것이 효과적이지 않았고 즉, Generator는 noise를 무시한다는 것을 알았다. 따라서 논문에서는 noise $z$를 Generator에 넣지 않고, Dropout을 사용하였다. 그럼에도 불구하고 drop out을 사용한 것에서는 minor stocahsticity만 발견되었고, noise를 사용한것과 같은 분포가 나오지 않았다. 이 부분은 이 연구에서 해결되지 않은 중요한 문제이다.


## 3.2 Network architectures

&nbsp;&nbsp; 논문에서는 DCGAN 구조를 채택하였고, generator, discriminator 모두 batch normalization에 소개 된 con-BatchNorm-ReLU 구조를 사용하였다. 


![Image5](/assets/images/pix2pix/image3.jpg){: .align-center}

### 3.2.1 Generator with skips

&nbsp;&nbsp; image-to-image translation에서 문제의 특징은 high-resolution input grid를 high-resoultion output grid로 mapping한다는 것이다. 이 문제에서 고려해야하는 문제는 input과 output가 표면적으로는 다르지만 같은 구조를 rendering해야한다는 것이다. 그러므로 input의 구조는 output의 구조와 약간 정렬되어 있어야한다. 논문에서는 이 사항을 중심으로 Generator architecture를 구성한다.
&nbsp;&nbsp; 많은 이전 연구들에서는 encoder-decoder network를 사용한다. 이 network는 input이 bottleneck layer까지 downsampling되는 layer를 통과한다. 이런 network는 bottleneck을 포함한 모든 layer를 통해 정보의 흐름이 전달 되어야한다. 많은 image translation 문제에서 input과 output 사이에 low-level 정보가 공유되고, network를 통해 직접적으로 전달되는 것이 이상적이다. 예를 들어 colorization에서는 input과 output이 확실한 edge의 위치 정보를 공유한다.
&nbsp;&nbsp; 또한 bottleneck으로 인해 정보가 손실되는 것을 방지하기 위해 Unet과 같이 skip connection을 사용하는데, 각 pair 사이의 skip connection을 추가한다. 

### 3.2.2 Markovian discriminator(Patch GAN)
![Image5](/assets/images/pix2pix/image4.jpg){: .align-center}
&nbsp;&nbsp; L2 loss, L1 loss가 image 생성에서 blur효과가 나타난다는 것은 잘 알려져있다. 그럼에도 불구하고 L1, L2 loss는 high frequency를 잘 잡아내지 못하지만, low frequencies에서는 정확하게 잡아낸다. 그래서 논문에서는 L1 term을 사용하고, high-frequency를 잡아내기 위해 GAN을 사용한다. 또한 local image patch를 사용하여 high-frequency를 잡아내며 각 $ N X N $의 patch가 real인지 분류하여 Discriminator의 output으로 반환한다.

&nbsp;&nbsp; 챕터 4.4에서는 $N$이 full size의 이미지보다 매우 작아도 high quality를 유지한다는 것을 증명하고, 작은 PatchGAN은 parameter수가 적으며, 빠르게 작동하고 큰 image에 대해서도 적용가능하다는 장점을 가지고 있다.

&nbsp;&nbsp; discriminator는 patch의 크기보다 먼 pixel간에는 독립적이라고 가정하며 Markov random filed에서 효과적이다.

## 3.3 Optimization and inference

&nbsp;&nbsp; 논문의 network를 최적화하기 위해 GAN의 approach를 따르며, $D,G$를 번갈하 가며 gradient descent를 진행한다. GAN에서는 $G$에서 $\log (1 - D(x, G(x, z)))$를 최소화 하도록 학습하는데, 논문에서는 $\log D(x,G(x,z)) $ 를 최대화 하는 방법을 사용하고, $D$의 loss함수를 2로 나누어 학습 속도를 늦춘다. training시에는 minibatch SGD를 사용하고 Adam optimizer를 사용하며, learning rate = 0.0002, momentum의 parameter는 $ \beta_1 = 0.5, \beta_2 = 0.999$ 이다.

&nbsp;&nbsp; inference를 할 때, generator network를 training phase와 정확히 같은 방법을 사용한다. 즉, test에서도 dropout을 사용하고 training batch가 아닌 test batch를 사용하여 batch normalization을 적용한다. batch size를 1로 설정하였을 떄 이것을 instance normalization이라고 하며 image generation에서 효과적이다. 실험에서는 batch 크기가 1에서 10의 값을 사용한다.

## 4. Experimetns

&nbsp;&nbsp; conditional GANs의 일반화 성능을 탐구하기 위해 다양한 task와 dataset에 대해 테스트를 진행하였다. task와 dataset은 다음과 같다.

* Semantic label $\leftrightarrow$ photo, **Cityscapes dataset**
* Architecture labels $\to$ photo, **CMP Facades**
* map $\leftrightarrow$ aerial photo, **Google Maps**
* BW $\to$ color photos
* Edge $\to$ photo
* Sketch $\to$ photo
* <span style='color:red'> Day $\to$ Night, **ACM TOG** </span>
* Thermal $\to$ color photos
* Photo with missing pixels $\to$ inpainted photo




&nbsp;&nbsp; 논문에서는 작은 dataset에서도 몇번 괜찮은 결과를 보였다고 나와있고, 그 중 facade dataset 400개의 image로 구성되어 있으며, day $\to$ night의 경우에는 91개의 image만으로도 좋은 결과를 보였다. 결과는 아래와 같다.

![Image5](/assets/images/pix2pix/image6.jpg){: .align-center}
![Image5](/assets/images/pix2pix/image7.jpg){: .align-center}

## 4.1 Evaluation metrics

&nbsp;&nbsp; 생성 image에 대한 quality 평가는 어려운 문제이며, MSE같은 전통적인 metric은 이미지의 주고를 고려하지 않기 때문에 사용하는데 한계가 있다. 따라서 논문에서는 2가지 전력을 사용한다.

1.  Amazon Mechanicl Turk(AMT)
    - real인지 fake인지 사람이 직접 평가하는 방법.
    - 이미지를 1초 동안 보여주며 피드백을 해주고 이후 test set에 대해 선택

2. FCN Score
    - 기존의 (semantic) classification model로 generated image에 대해 구분을 할 수 있는지에 대한 지표
    - 이미지가 현실적인지 판단
    - Cityscapes에서 metric으로도 쓰임

## 4.2 Analysis of the objective function

&nbsp;&nbsp; ablation study를 통해서 L1과 GAN term의 영향을 독립적으로 분리하고, conditional discriminator와 unconditional discriminator를 비교한다. 

![Image5](/assets/images/pix2pix/image5.jpg){: .align-center}

&nbsp;&nbsp; 위 그림을 보면 label $\to$ photo에서 quality적 효과를 보여준다. L1 또한 reasonable하지만 blur가 생기는 것을 볼 수 있고, cGAN ($\lambda = 0$)만을 사용하였을 때는 sharp하지만 visual artifacts가 존재하는 것을 볼 수 있다. 두 term을 모두($\lambda = 100$) 사용하였을 때는 이러한 artifacts가 줄어드는 것을 확인할 수 있다.
&nbsp;&nbsp; 수치적인 비교를 위해 FCN score를 확인해보면 아래 표와 같다. loss에 따른 FCN score를 확인해보면 모두 L1과 GAN을 함께 사용했을 때 수치가 높게 나왔고, 이것은 Ground truth에 가깝다는 것을 확인할 수 있다.

![Image5](/assets/images/pix2pix/image8.jpg){: .align-center}

## 4.3 Analysis of the generator architecture

![Image5](/assets/images/pix2pix/image9.jpg){: .align-center}

&nbsp;&nbsp;figure 5를 보았을 때 U-net의 skip connection의 효과를 확인할 수 있다. 또한 Encoder-Decoder 구조와 U-net 구조에서 모두 L1과 GAN을 결합하여 사용한 것이 더 선명하게 나오는 것을 확인할 수 있다. 또한 정량적으로 보더라도 U-net(L1+cGAN)을 사용하였을 때가 가장 좋은 성능을 보이는 것을 볼 수 있다.

## 4.4 From PixelGANs to PatchGANs to ImageGANs

&nbsp;&nbsp; 이 챕터에서는 discriminator의 patch size를 1 X 1인 PixelGAN부터 286 X 286으로 바꾸었을 때의 영향을 test한다. 
![Image5](/assets/images/pix2pix/image10.jpg){: .align-center}
![Image5](/assets/images/pix2pix/image11.jpg){: .align-center}

위 그림에서 볼 수 있듯이, pixelGAN의 경우에는 spatial sharpness를 L1보다 올려주지는 못하지만 색상에서의 다양성을 증가시킨다. 16X16 PatchGAN을 사용할 때는 image가 조금 더 sharp해진것을 확인할 수 있으며, 좋은 성능을 보이지만, tiling artifacts가 존재한다. 70X70 PatchGAN에서는 이러한 artifacts를 완화시키며 가장 좋은 성능을 달성하는 것을 볼 수 있다. 논문에서는 언급하지 않는 한 모두 70X70의 PatchGAN을 사용한다. 286X286 ImageGAN에서는 visual적인 향상이 거의 없는 것을 볼 수 있다.

## 4.5 Perceptual validation

&nbsp;&nbsp; map $\leftrightarrow$ aerial photograph, grayscale $\to$ color의 task에서 인지성을 검증한다. L1과 cGAN을 결합하여 사용했을 때는 L1보다 훨씬 많은 실험자들을 속였고, Photo $\to$ map 에서는 기하학적으로 큰 차이는 보이지 않는것을 증명하였다.

![Image5](/assets/images/pix2pix/image12.jpg){: .align-center}

## 4.6 Semantic Segmentation

&nbsp;&nbsp; Conditional GAN은 image 처리에서 output이 꽤 detail해야하는데, semantic segmentation에서도 이 방법이 효과적인지 보인다. 이 실험은 Cityscapes dataset을 통해 학습을 시키고 photo $\to$ label이기 때문에, output이 input에 비해 덜 복잡한 구조를 가진다. 결과는 다음과 같다. 이 방법의 경우에는 오히려 L1만 사용한 것이 가장 좋은 성능을 보였다. 또한 L1을 함께 사용하지 않고 cGAN으로만 학습을 시켰을 때도 괜찮은 결과를 보였다.

![Image5](/assets/images/pix2pix/image13.jpg){: .align-center}
![Image5](/assets/images/pix2pix/image14.jpg){: .align-center}


# 5. Conclusion

&nbsp;&nbsp; 이 논문의 결과는 conditional GAN이 많은 image translation, 특히 구조화된 graphic을 output으로 내보낼 때, 좋은 성능을 보인다. 또한 각 data와 task에 맞게 loss가 적응하므로, 다양한 setting에서도 사용될 수 있다는 전망이 있다.